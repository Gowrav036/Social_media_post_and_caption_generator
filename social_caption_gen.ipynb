{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "FiWkvkdqQ2Ji",
        "outputId": "e34e4990-5e32-4bb4-f173-173895c97029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key: AIzaSyCQYD065v9_SZOzavaFApRL99p1yR3ujBM\n",
            "âœ… Gemini model initialized successfully.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a7dc019419a9d2161c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a7dc019419a9d2161c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a7dc019419a9d2161c.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install -q google-generativeai gradio\n",
        "\n",
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "\n",
        "# Prefer environment variable; fall back to prompt\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\") or input(\"Enter your Gemini API key: \").strip()\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Global model object with lazy init and fallback across common model IDs\n",
        "gemini_model = None\n",
        "\n",
        "def _pick_model():\n",
        "    candidates = [\n",
        "        \"gemini-1.5-flash\",\n",
        "        \"gemini-1.5-flash-8b\",\n",
        "        \"gemini-1.5-pro\",\n",
        "        \"gemini-1.0-pro\",\n",
        "        \"gemini-pro\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for name in candidates:\n",
        "        try:\n",
        "            m = genai.GenerativeModel(name)\n",
        "            _ = m.generate_content(\"ping\")\n",
        "            print(f\"Using model: {name}\")\n",
        "            return m\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "    raise RuntimeError(f\"No available Gemini model from candidates. Last error: {last_err}\")\n",
        "\n",
        "try:\n",
        "    gemini_model = _pick_model()\n",
        "    print(\"Gemini model initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini: {e}\")\n",
        "\n",
        "\n",
        "def generate_social_media_post(keywords, tone, platform, hashtag_count):\n",
        "    global gemini_model\n",
        "    if gemini_model is None:\n",
        "        try:\n",
        "            gemini_model = _pick_model()\n",
        "        except Exception as e:\n",
        "            return f\"Model initialization failed: {e}\"\n",
        "\n",
        "    if not keywords:\n",
        "        return \"Please enter some keywords.\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"Generate a {tone} social media caption for {platform} with keywords: {keywords}. \"\n",
        "        f\"Include exactly {hashtag_count} hashtags and a few emojis. Format as Caption, Hashtags, Emojis.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(\"Calling Gemini with:\", {\n",
        "            \"tone\": tone,\n",
        "            \"platform\": platform,\n",
        "            \"hashtag_count\": hashtag_count,\n",
        "            \"keywords_len\": len(keywords or \"\")\n",
        "        })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        last_err = None\n",
        "        for _ in range(2):\n",
        "            try:\n",
        "                response = gemini_model.generate_content(prompt)\n",
        "                if hasattr(response, \"text\") and response.text:\n",
        "                    return response.text\n",
        "                try:\n",
        "                    cands = getattr(response, \"candidates\", []) or []\n",
        "                    parts = []\n",
        "                    for c in cands:\n",
        "                        content = getattr(c, \"content\", None)\n",
        "                        if content and hasattr(content, \"parts\"):\n",
        "                            for p in content.parts:\n",
        "                                if hasattr(p, \"text\") and p.text:\n",
        "                                    parts.append(p.text)\n",
        "                    if parts:\n",
        "                        return \"\\n\".join(parts)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                return \"The model returned an empty response. Please try different inputs or try again.\"\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                time.sleep(2)\n",
        "        return f\"Failed to generate content after retries. Error: {last_err}\"\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {e}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Social Media Post Generator\")\n",
        "\n",
        "    with gr.Row():\n",
        "        keywords_input = gr.Textbox(label=\"Keywords\", placeholder=\"e.g., coffee morning vibes\")\n",
        "        tone_input = gr.Dropdown([\"Funny\", \"Professional\", \"Motivational\", \"Casual\", \"Inspirational\"], value=\"Casual\", label=\"Tone\")\n",
        "\n",
        "    with gr.Row():\n",
        "        platform_input = gr.Dropdown([\"Instagram\", \"LinkedIn\", \"Twitter\"], value=\"Instagram\", label=\"Platform\")\n",
        "        hashtag_input = gr.Dropdown([\"3\", \"5\", \"10\"], value=\"5\", label=\"Number of Hashtags\")\n",
        "\n",
        "    output_box = gr.Textbox(label=\"Generated Caption, Hashtags & Emojis\", lines=8)\n",
        "\n",
        "    generate_btn = gr.Button(\"Generate Post\")\n",
        "    generate_btn.click(\n",
        "        fn=generate_social_media_post,\n",
        "        inputs=[keywords_input, tone_input, platform_input, hashtag_input],\n",
        "        outputs=output_box\n",
        "    )\n",
        "\n",
        "# Use port from env if present; else let Gradio choose automatically\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port=int(os.getenv(\"GRADIO_SERVER_PORT\") or 0), debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
